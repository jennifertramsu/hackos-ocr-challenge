{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from pathlib import Path\n",
    "from keras.layers import TextVectorization\n",
    "from keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv1D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    GlobalAveragePooling1D,\n",
    "    MaxPooling1D,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = Path('../data/raw')\n",
    "TRAIN = DATA / 'train'\n",
    "TEST = DATA / 'test'\n",
    "MODELS = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 139804 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "CLASSES = [f.stem for f in TRAIN.glob('*')]\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "TRAIN_DF = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    TRAIN,\n",
    "    batch_size=BATCH,\n",
    "    class_names=CLASSES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 't', 'a', 'g', 'c', 'n']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text Vectorization\n",
    "# MAX_VOCAB = 64\n",
    "MAX_LEN = 500\n",
    "text_vectorizer = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    split=lambda x: tf.strings.unicode_split(x, \"UTF-8\")\n",
    ")\n",
    "\n",
    "text_vectorizer.adapt(TRAIN_DF.map(lambda x, y: x))\n",
    "VOCAB_SIZE = len(text_vectorizer.get_vocabulary())\n",
    "text_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return text_vectorizer(text)-2, label\n",
    "\n",
    "train_data = TRAIN_DF.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 534)\n",
      "(64,)\n"
     ]
    }
   ],
   "source": [
    "# Check shape\n",
    "for text, label in train_data.take(1):\n",
    "    print(text.shape)\n",
    "    print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model\n",
    "# class CharCNN(tf.keras.Model):\n",
    "#     def __init__(self, vocab_size, num_classes):\n",
    "#         super(CharCNN, self).__init__()\n",
    "#         self.onehot = keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x, \"int64\"), vocab_size)),\n",
    "#         self.conv1 = tf.keras.layers.Conv1D(32, kernel_size=8, activation='relu')\n",
    "#         self.norm = keras.layers.BatchNormalization(),\n",
    "#         self.conv2 = tf.keras.layers.Conv1D(16, kernel_size=8, activation='relu')\n",
    "#         self.pool = tf.keras.layers.MaxPooling1D(),\n",
    "#         self.conv3 = tf.keras.layers.Conv1D(4, kernel_size=8, activation='relu')\n",
    "#         self.avgpool = tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#         self.drop = tf.keras.layers.Dropout(0.3),\n",
    "#         self.fc = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "#     def call(self, x):\n",
    "#         x = self.onehot(x),\n",
    "#         x = self.conv1(x),\n",
    "#         x = self.norm(x),\n",
    "#         x = self.conv2(x),\n",
    "#         x = self.norm(x),\n",
    "#         x = self.pool(x),\n",
    "#         x = self.conv3(x),\n",
    "#         x = self.norm(x),\n",
    "#         x = self.pool(x),\n",
    "#         x = self.drop(x),\n",
    "#         x = self.avgpool(x),\n",
    "#         return self.fc(x)\n",
    "    \n",
    "# model = CharCNN(VOCAB_SIZE, NUM_CLASSES)\n",
    "onehot_layer = tf.keras.layers.Lambda(lambda x: tf.one_hot(tf.cast(x, \"int64\"), VOCAB_SIZE))\n",
    "last_layer = Dense(1, activation='sigmoid')\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    onehot_layer,\n",
    "    Conv1D(32, kernel_size=8, data_format=\"channels_last\", activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(),\n",
    "    Conv1D(16, kernel_size=8, data_format=\"channels_last\", activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(),\n",
    "    Conv1D(4, kernel_size=8, data_format=\"channels_last\", activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(),\n",
    "    Dropout(0.3),\n",
    "    GlobalAveragePooling1D(),\n",
    "    last_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 31ms/step - accuracy: 0.6329 - loss: 0.6374\n",
      "Epoch 2/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 29ms/step - accuracy: 0.6890 - loss: 0.5863\n",
      "Epoch 3/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 29ms/step - accuracy: 0.7061 - loss: 0.5657\n",
      "Epoch 4/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 37ms/step - accuracy: 0.7149 - loss: 0.5542\n",
      "Epoch 5/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 45ms/step - accuracy: 0.7226 - loss: 0.5457\n",
      "Epoch 6/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 41ms/step - accuracy: 0.7296 - loss: 0.5391\n",
      "Epoch 7/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 251ms/step - accuracy: 0.7323 - loss: 0.5342\n",
      "Epoch 8/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 25ms/step - accuracy: 0.7359 - loss: 0.5306\n",
      "Epoch 9/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 28ms/step - accuracy: 0.7373 - loss: 0.5278\n",
      "Epoch 10/10\n",
      "\u001b[1m2185/2185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 32ms/step - accuracy: 0.7401 - loss: 0.5249\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fbb6c78fdd0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting model\n",
    "EPOCHS = 10\n",
    "model.fit(train_data, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 34952 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    TEST,\n",
    "    batch_size=BATCH,\n",
    "    class_names=CLASSES\n",
    ").map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m547/547\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - accuracy: 0.6250 - loss: 0.7853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7829174995422363, 0.6252861022949219]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(MODELS / 'char_cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
